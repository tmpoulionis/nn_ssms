{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5b75c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/tmpoulionis/miniconda3/envs/mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from models.test_model import MambaModel\n",
    "from utils.lightning import LightningMamba\n",
    "from config import get_config\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from utils.utils import set_seed, model_summary, format_time, handle_wandb_login\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873c46be",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Set seed for reproducibility\n",
    "if config[\"seed\"]:\n",
    "    set_seed(config[\"seed\"])\n",
    "\n",
    "# Parse config\n",
    "MODEL_CONFIG = config[\"model\"]\n",
    "TRAINER_CONFIG = config[\"trainer\"]\n",
    "DATASET_CONFIG = config[\"dataset\"]\n",
    "OPTIMIZER_CONFIG = config[\"optimizer\"]\n",
    "WANDB_CONFIG = config[\"wandb\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98c6f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Loading sc09 dataset...\n",
      "\t Creating DataLoaders...\n",
      "\t Dataloaders created.\n",
      "  ✓ Dataset: sc09\n",
      "  ✓ Classes: 10\n",
      "  ✓ Input shape: torch.Size([128, 107, 64])\n",
      "  ✓ Features: 64\n",
      "  ✓ Sequence Length: 107\n"
     ]
    }
   ],
   "source": [
    "# ------- Load Dataset and create DataLoaders -------\n",
    "data = data.get_dataloaders(**DATASET_CONFIG)\n",
    "train_loader = data[\"train_loader\"]\n",
    "val_loader = data[\"val_loader\"]\n",
    "test_loader = data[\"test_loader\"]\n",
    "num_classes = data[\"num_classes\"]\n",
    "if TRAINER_CONFIG[\"max_epochs\"] is not None:\n",
    "    total_steps = len(train_loader) * TRAINER_CONFIG[\"max_epochs\"]\n",
    "    if TRAINER_CONFIG[\"max_steps\"] is not None:\n",
    "        total_steps = min(total_steps, TRAINER_CONFIG[\"max_steps\"])\n",
    "else:\n",
    "    try:\n",
    "        total_steps = TRAINER_CONFIG[\"max_steps\"]\n",
    "    except: \n",
    "        raise ValueError(\"Either max_steps or max_epochs must be defined.\")\n",
    "    \n",
    "print(f\"  ✓ Dataset: {DATASET_CONFIG['dataset_name']}\")\n",
    "print(f\"  ✓ Classes: {data['num_classes']}\")\n",
    "print(f\"  ✓ Input shape: {data['input_shape']}\")\n",
    "print(f\"  ✓ Features: {data['feature_dim']}\")\n",
    "print(f\"  ✓ Sequence Length: {data['sequence_length']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f704894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MambaModel                               --\n",
       "├─ModuleList: 1-1                        --\n",
       "│    └─PhotonicMamba: 2-1                --\n",
       "│    │    └─Mamba: 3-1                   32,640\n",
       "├─ModuleList: 1-2                        --\n",
       "│    └─LayerNorm: 2-2                    128\n",
       "├─LayerNorm: 1-3                         128\n",
       "├─Sequential: 1-4                        --\n",
       "│    └─Linear: 2-3                       8,320\n",
       "│    └─LayerNorm: 2-4                    256\n",
       "│    └─GELU: 2-5                         --\n",
       "│    └─Dropout: 2-6                      --\n",
       "│    └─Linear: 2-7                       1,290\n",
       "=================================================================\n",
       "Total params: 42,762\n",
       "Trainable params: 42,762\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------- Create Model -------\n",
    "print(\"Constructing Model...\")\n",
    "model = MambaModel(**MODEL_CONFIG, d_out=num_classes).cuda()\n",
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0b9de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/6] Setting up W&B Logger...\n",
      "\n",
      "--- Weights & Biases (W&B) Configuration ---\n",
      "W&B username: tmpoulionis-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtmpoulionis\u001b[0m (\u001b[33mtmpoulionis-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B login successful!\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb_logs/wandb/run-20251120_135441-n7xq75va</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmpoulionis-/lightning_logs/runs/n7xq75va' target=\"_blank\">splendid-silence-44</a></strong> to <a href='https://wandb.ai/tmpoulionis-/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmpoulionis-/lightning_logs' target=\"_blank\">https://wandb.ai/tmpoulionis-/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmpoulionis-/lightning_logs/runs/n7xq75va' target=\"_blank\">https://wandb.ai/tmpoulionis-/lightning_logs/runs/n7xq75va</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Project: lightning_logs\n",
      "  ✓ Name: splendid-silence-44\n",
      "  ✓ URL: https://wandb.ai/tmpoulionis-/lightning_logs/runs/n7xq75va\n"
     ]
    }
   ],
   "source": [
    "# ------- W&B Logger -------\n",
    "print(\"\\n[3/6] Setting up W&B Logger...\")\n",
    "usrname = handle_wandb_login()\n",
    "wandb_logger = WandbLogger(\n",
    "    project=WANDB_CONFIG[\"project\"],\n",
    "    entity=usrname,\n",
    "    name=WANDB_CONFIG[\"name\"],\n",
    "    log_model=\"all\",\n",
    "    save_dir=\"./wandb_logs\"\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Project:\", wandb_logger.experiment.project)\n",
    "print(f\"  ✓ Name:\", wandb_logger.experiment.name)\n",
    "print(f\"  ✓ URL:\", wandb_logger.experiment.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a51c1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/6] Setting up Callbacks...\n",
      "  ✓ Learning rate monitor\n",
      "  ✓ Model checkpointing (save top 3)\n",
      "  ✓ Early stopping (patience=20)\n"
     ]
    }
   ],
   "source": [
    "# ------- Callbacks -------\n",
    "print(\"\\n[4/6] Setting up Callbacks...\")\n",
    "\n",
    "callbacks = [\n",
    "    LearningRateMonitor(logging_interval='step'),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=f\"./checkpoints/{wandb_logger.name}\",\n",
    "        filename=\"best-{epoch:02d}-{val/acc:.4f}\",\n",
    "        monitor=\"val/acc\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=1,\n",
    "        save_last=True\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val/loss\",\n",
    "        patience=20,\n",
    "        mode=\"min\",\n",
    "        verbose=True\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"  ✓ Learning rate monitor\")\n",
    "print(f\"  ✓ Model checkpointing (save top 3)\")\n",
    "print(f\"  ✓ Early stopping (patience=20)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca97fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Scheduler -------\n",
    "from train import create_scheduler\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "scheduler_config = {\n",
    "    \"scheduler\": create_scheduler,\n",
    "    \"params\": {\n",
    "        \"total_steps\": total_steps,\n",
    "        \"warmup_steps\": warmup_steps\n",
    "    }\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf18a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/6) Setting up Lightning Module...\n"
     ]
    }
   ],
   "source": [
    "# ------- Lightning Module -------\n",
    "print(\"\\n[5/6) Setting up Lightning Module...\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "lightning_module = LightningMamba(\n",
    "    model=model,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    loss_fn=loss_fn,\n",
    "    opt_hyperparams=OPTIMIZER_CONFIG,\n",
    "    scheduler_config=scheduler_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72506922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/6] Initializing Trainer...\n",
      "  ✓ Max steps: 200000\n",
      "  ✓ Max epochs: 30\n",
      "  ✓ Accelerator: auto\n",
      "  ✓ Gradient clip: 0.1\n"
     ]
    }
   ],
   "source": [
    "# ------- Trainer -------\n",
    "print(\"\\n[6/6] Initializing Trainer...\")\n",
    "trainer = L.Trainer(\n",
    "    **TRAINER_CONFIG,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Max steps: {TRAINER_CONFIG['max_steps'] if TRAINER_CONFIG['max_steps'] else 'N/A'}\")\n",
    "print(f\"  ✓ Max epochs: {TRAINER_CONFIG['max_epochs'] if TRAINER_CONFIG['max_epochs'] else 'N/A'}\")\n",
    "print(f\"  ✓ Accelerator: {TRAINER_CONFIG['accelerator']}\")\n",
    "print(f\"  ✓ Gradient clip: {TRAINER_CONFIG['gradient_clip_val']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c509ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/tmpoulionis/miniconda3/envs/mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.data import create_dataset\n",
    "\n",
    "dataset = create_dataset('hg38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1198ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <dataloaders.datasets.hg38_dataset.HG38Dataset at 0x7fb1f76ef700>,\n",
       " 'valid': <dataloaders.datasets.hg38_dataset.HG38Dataset at 0x7fb1f6d1dcf0>,\n",
       " 'test': <dataloaders.datasets.hg38_dataset.HG38Dataset at 0x7fb2103ac9a0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7654e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/tmpoulionis/miniconda3/envs/mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataloaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_dataloaders\n\u001b[0;32m----> 3\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeechcommands\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/tmpoulionis/nn_mamba/dataloaders/data.py:24\u001b[0m, in \u001b[0;36mget_dataloaders\u001b[0;34m(dataset_name, root, batch_size, num_workers, pin_memory, **dataset_kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(DATASET_REGISTRY\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Retrieve data subsets from the registry\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m data_config \u001b[38;5;241m=\u001b[39m DATASET_REGISTRY[dataset_name\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[1;32m     27\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/media/data/tmpoulionis/nn_mamba/dataloaders/data.py:67\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(dataset_name, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m subsets:\n\u001b[0;32m---> 67\u001b[0m     datasets[split] \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n",
      "File \u001b[0;32m/media/data/tmpoulionis/nn_mamba/dataloaders/datasets/sc_dataset.py:40\u001b[0m, in \u001b[0;36mSCDataset.__init__\u001b[0;34m(self, subset, root, mel_transform, n_mels, n_fft, hop_length, filter_labels)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m all_labels\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSCDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__len__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_to_index \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     43\u001b[0m     label: i \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels)\n\u001b[1;32m     44\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from dataloaders.data import get_dataloaders\n",
    "\n",
    "dataloaders = get_dataloaders(\"speechcommands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610a75b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_loader': <torch.utils.data.dataloader.DataLoader at 0x7f7619d35900>,\n",
       " 'validation_loader': <torch.utils.data.dataloader.DataLoader at 0x7f74fe616fe0>,\n",
       " 'testing_loader': <torch.utils.data.dataloader.DataLoader at 0x7f74fe617250>,\n",
       " 'input_shape': [64, 107]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b717cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
